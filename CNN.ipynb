{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Jithin Pradeep\"\n",
    "__copyright__ = \"Copyright (C) 2018 Jithin Pradeep\"\n",
    "__license__ = \"MIT License\"\n",
    "__version__ = \"1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def label2int(ch):\n",
    "    asciiVal = ord(ch)\n",
    "    if(asciiVal<=57): #0-9\n",
    "        asciiVal-=48\n",
    "    elif(asciiVal<=90): #A-Z\n",
    "        asciiVal-=55\n",
    "    else: #a-z\n",
    "        asciiVal-=61\n",
    "    return asciiVal\n",
    "    \n",
    "def int2label(i):\n",
    "    if(i<=9): #0-9\n",
    "        i+=48\n",
    "    elif(i<=35): #A-Z\n",
    "        i+=55\n",
    "    else: #a-z\n",
    "        i+=61\n",
    "    return chr(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "from natsort import natsorted\n",
    "\n",
    "\n",
    "# Path of data files\n",
    "path = \"../data\"\n",
    "\n",
    "# Input image dimensions\n",
    "img_rows, img_cols = 32,32 \n",
    "\n",
    "# Keep or not the initial image aspect ratio\n",
    "keepRatio = False\n",
    "\n",
    "# Suffix of the created directories and files\n",
    "suffix = \"Preproc_\" + str(img_rows) + \"_\"+ str(img_cols) + (\"_kr\" if keepRatio else \"\")\n",
    "\n",
    "# Create the directories if needed\n",
    "if not os.path.exists(os.getcwd()+\"/data/train\"+suffix ):\n",
    "    os.makedirs(os.getcwd()+\"/data/train\"+suffix)\n",
    "if not os.path.exists( os.getcwd()+\"/data/test\"+suffix ):\n",
    "    os.makedirs(os.getcwd()+\"/data/test\"+suffix)\n",
    "    \n",
    "    \n",
    "### Images preprocessing ###\n",
    "\n",
    "for setType in [\"train\", \"test\"]:\n",
    "    # We have to make sure files are sorted according to labels, even if they don't have trailing zeros\n",
    "    files = natsorted(glob.glob(os.getcwd()+\"/data/\"+setType+\"/*\"))\n",
    "    #print(files) \n",
    "    data = np.zeros((len(files), img_rows, img_cols)) #will add the channel dimension later\n",
    "    \n",
    "    for i, filepath in enumerate(files):\n",
    "        #print(filepath)\n",
    "        image = imread(filepath, True) #True: flatten to grayscale\n",
    "        if keepRatio:\n",
    "            # Find the largest dimension (height or width)\n",
    "            maxSize = max(image.shape[0], image.shape[1])\n",
    "            \n",
    "            # Size of the resized image, keeping aspect ratio\n",
    "            imageWidth = math.floor(img_rows*image.shape[0]/maxSize)\n",
    "            imageHeigh = math.floor(img_cols*image.shape[1]/maxSize)\n",
    "            \n",
    "            # Compute deltas to center image (should be 0 for the largest dimension)\n",
    "            dRows = (img_rows-imageWidth)//2\n",
    "            dCols = (img_cols-imageHeigh)//2\n",
    "                        \n",
    "            imageResized = np.zeros((img_rows, img_cols))\n",
    "            imageResized[dRows:dRows+imageWidth, dCols:dCols+imageHeigh] = imresize(image, (imageWidth, imageHeigh))\n",
    "            \n",
    "            # Fill the empty image with the median value of the border pixels\n",
    "            # This value should be close to the background color\n",
    "            val = np.median(np.append(imageResized[dRows,:],\n",
    "                                      (imageResized[dRows+imageWidth-1,:],\n",
    "                                      imageResized[:,dCols],\n",
    "                                      imageResized[:,dCols+imageHeigh-1])))\n",
    "                                      \n",
    "            # If rows were left blank\n",
    "            if(dRows>0):\n",
    "                imageResized[0:dRows,:].fill(val)\n",
    "                imageResized[dRows+imageWidth:,:].fill(val)\n",
    "                \n",
    "            # If columns were left blank\n",
    "            if(dCols>0):\n",
    "                imageResized[:,0:dCols].fill(val)\n",
    "                imageResized[:,dCols+imageHeigh:].fill(val)\n",
    "        else:\n",
    "            imageResized = imresize(image, (img_rows, img_cols))\n",
    "        \n",
    "        # Add the resized image to the dataset\n",
    "        data[i] = imageResized\n",
    "        \n",
    "        #Save image (mostly for visualization)\n",
    "        #filename = filepath.split(\"\\\\\")[-1]\n",
    "        #print(filname)\n",
    "        #filenameDotSplit = filename.split(\".\")\n",
    "        #print(filenameDotSplit)\n",
    "        #newFilename = str((filenameDotSplit[0])).zfill(5) + \".\" + filenameDotSplit[-1].lower()  #Add trailing zeros\n",
    "        #print(str((filenameDotSplit[0])))\n",
    "        #print(newFilename)\n",
    "        #newName = \"/\".join(filepath.split(\"\\\\\")[:-1] ) +suffix+ \"/\" + newFilename\n",
    "        # print(newName)\n",
    "        # imsave(newName, imageResized)\n",
    "        \n",
    "    # Add channel/filter dimension\n",
    "    data = data[:,np.newaxis,:,:] \n",
    "    #data = data[:,:,:,np.newaxis]\n",
    "    \n",
    "    # Makes values floats between 0 and 1 (gives better results for neural nets)\n",
    "    data = data.astype('float32')\n",
    "    data /= 255\n",
    "    \n",
    "    # Save the data as numpy file for faster loading\n",
    "    np.save(os.getcwd()+\"/data/\"+setType+suffix+\".npy\", data)\n",
    "\n",
    "    \n",
    "### Labels preprocessing ###\n",
    "\n",
    "# Load labels\n",
    "y_train = pd.read_csv(os.getcwd()+\"/data/trainLabels.csv\").values[:,1] #Keep only label\n",
    "\n",
    "# Convert labels to one-hot vectors\n",
    "Y_train = np.zeros((y_train.shape[0], len(np.unique(y_train))))\n",
    "\n",
    "for i in range(y_train.shape[0]):\n",
    "    Y_train[i][label2int(y_train[i])] = 1 # One-hot\n",
    "\n",
    "# Save preprocessed label to nupy file for faster loading\n",
    "np.save(os.getcwd()+\"/data/\"+\"labelsPreproc.npy\", Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "\n",
    "batch_size = 400\n",
    "nb_classes = 62 # A-Z, a-z and 0-9\n",
    "nb_epoch = 100\n",
    "\n",
    "# Input image dimensions\n",
    "img_rows, img_cols = 48, 48\n",
    "\n",
    "# Path of data files\n",
    "path = \"../data\"\n",
    "\n",
    "# Load the preprocessed data and labels\n",
    "X_train_all = np.load(os.getcwd()+\"/data/\"+\"/trainPreproc_\"+str(img_rows)+\"_\"+str(img_cols)+\".npy\")\n",
    "Y_train_all = np.load(os.getcwd()+\"/data/\"+\"/labelsPreproc.npy\")\n",
    "\n",
    "# Do multiple learnings and predictions with the aim of averaging them\n",
    "for runID in range (18):    \n",
    "    # Split in train and validation sets to get the \"best\" model.\n",
    "    X_train, X_val, Y_train, Y_val = \\\n",
    "        train_test_split(X_train_all, Y_train_all, test_size=0.25, stratify=np.argmax(Y_train_all, axis=1))\n",
    "   \n",
    "                             \n",
    "    # Parametrize the image augmentation class\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range = 25,\n",
    "        zca_whitening=True,\n",
    "        width_shift_range = 0.15,\n",
    "        height_shift_range = 0.15,\n",
    "        shear_range = 0.4,\n",
    "        zoom_range = 0.4, \n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        channel_shift_range = 0.2,\n",
    "        channel_flip = True, # You must modify the ImageDataGenerator class for that parameter to work\n",
    "        channel_flip_max = 1.) # You must modify the ImageDataGenerator class for that parameter to work\n",
    "    #datagen.fit(X_train)\n",
    "    \n",
    "    ### CNN MODEL ###\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution2D(128, 3, 3, border_mode='same', init='he_normal', activation = 'relu', input_shape=(1,img_rows, img_cols)))\n",
    "    model.add(Convolution2D(128, 3, 3, border_mode='same', init='he_normal', activation = 'relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Convolution2D(256, 3, 3, border_mode='same', init='he_normal', activation = 'relu'))\n",
    "    model.add(Convolution2D(256, 3, 3, border_mode='same', init='he_normal', activation = 'relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Convolution2D(512, 3, 3, border_mode='same', init='he_normal', activation = 'relu'))\n",
    "    model.add(Convolution2D(512, 3, 3, border_mode='same', init='he_normal', activation = 'relu'))\n",
    "    model.add(Convolution2D(512, 3, 3, border_mode='same', init='he_normal', activation = 'relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, init='he_normal', activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, init='he_normal', activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes, init='he_normal', activation = 'softmax'))\n",
    "\n",
    "    ### LEARNING ###\n",
    "\n",
    "    # First, use AdaDelta for some epochs because AdaMax gets stuck\n",
    "    lrate = 0.001\n",
    "    decay = lrate/30\n",
    "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "    #adamax = Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='AdaDelta',  \n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "\n",
    "                  \n",
    "    # 20 epochs is sufficient\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size,\n",
    "                        nb_epoch=30, \n",
    "                        validation_data=(X_val, Y_val),\n",
    "                        verbose=1)\n",
    "                      \n",
    "    # Now, use AdaMax\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                 optimizer='AdaMax',  \n",
    "                 metrics=[\"accuracy\"])\n",
    "\n",
    "    # We want to keep the best model. This callback will store \n",
    "    # in a file the weights of the model with the highest validation accuracy  \n",
    "    saveBestModel = ModelCheckpoint(\"best.kerasModelWeights64\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make the model learn using the image generator\n",
    "    model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "                        samples_per_epoch=len(X_train),\n",
    "                        nb_epoch=nb_epoch, \n",
    "                        validation_data=(X_val, Y_val),\n",
    "                        callbacks=[saveBestModel],\n",
    "                        verbose=1)\n",
    "\n",
    "    ### PREDICTION ###\n",
    "                        \n",
    "    # Load the model with the highest validation accuracy\n",
    "    model.load_weights(\"best.kerasModelWeights\")\n",
    "\n",
    "    # Load Kaggle test set\n",
    "    X_test = np.load(os.getcwd()+\"/data/testPreproc_\"+str(img_rows)+\"_\"+str(img_cols)+\".npy\")\n",
    "\n",
    "    # Predict the class (give the index in the one-hot vector of the most probable class)\n",
    "    Y_test_pred = model.predict_classes(X_test)\n",
    "    \n",
    "    # Translate integers to character labels\n",
    "    vInt2label = np.vectorize(int2label)\n",
    "    Y_test_pred = vInt2label(Y_test_pred)\n",
    "    \n",
    "    # Save the predicitions in Kaggle format\n",
    "    np.savetxt(os.getcwd()+\"/data/CNN_pred_\"+str(runID)+\".csv\", np.c_[range(6284,len(Y_test_pred)+6284),Y_test_pred], delimiter=',', header = 'ID,Class', comments = '', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"best.kerasModelWeights\")\n",
    "\n",
    "    # Load Kaggle test set\n",
    "X_test = np.load(os.getcwd()+\"/data/testPreproc_\"+str(img_rows)+\"_\"+str(img_cols)+\".npy\")\n",
    "\n",
    "    # Predict the class (give the index in the one-hot vector of the most probable class)\n",
    "Y_test_pred = model.predict_classes(X_test)\n",
    "    \n",
    "    # Translate integers to character labels\n",
    "vInt2label = np.vectorize(int2label)\n",
    "Y_test_pred = vInt2label(Y_test_pred)\n",
    "    \n",
    "    # Save the predicitions in Kaggle format\n",
    "np.savetxt(os.getcwd()+\"/data/CNN_pred_\"+str(runID)+\".csv\", np.c_[range(6284,len(Y_test_pred)+6284),Y_test_pred], delimiter=',', header = 'ID,Class', comments = '', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Path of data files\n",
    "path = \"../data\"\n",
    "\n",
    "\n",
    "# List of prediction files in path\n",
    "allPredsFiles = glob.glob(os.getcwd()+\"/data/CNN_pred_*.csv\")\n",
    "\n",
    "# Array containg all the prediction\n",
    "allPreds = []\n",
    "\n",
    "for file in allPredsFiles:\n",
    "    preds = pd.read_csv(file).values\n",
    "    predictionTemplate = preds # Keep one prediction as template\n",
    "    allPreds.append(preds[:,1:2]) #1:2 keeps dimension and removes label\n",
    "\n",
    "# Stacks all the predictions (easier to treat this way)\n",
    "predsStacked = np.hstack(allPreds) \n",
    "\n",
    "# Create the averaged result array, copying the template\n",
    "predsAveraged = np.array(predictionTemplate)\n",
    "\n",
    "# For each prediction, get the most frequent one\n",
    "for i in range(predsStacked.shape[0]):\n",
    "    (values,counts) = np.unique(predsStacked[i], return_counts=True) # Use count to find most frequent one\n",
    "    ind=np.argmax(counts) # Index of the most frequent label\n",
    "    predsAveraged[i,1] = values[ind]  # gets the most frequent label\n",
    "    \n",
    "# Save the averaged predicitions using Kaggle format\n",
    "np.savetxt(os.getcwd()+\"/data/avg_pred.csv\", np.c_[predsAveraged], delimiter=',', header = 'ID,Class', comments = '', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
