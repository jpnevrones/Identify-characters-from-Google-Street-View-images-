{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Jithin Pradeep\"\n",
    "__copyright__ = \"Copyright (C) 2018 Jithin Pradeep\"\n",
    "__license__ = \"MIT License\"\n",
    "__version__ = \"1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "from skimage import data, io, filters\n",
    "from skimage.filters import threshold_otsu\n",
    "image_width = 20\n",
    "image_height = 20\n",
    "\n",
    "#Function for one hot code encoding implementation\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    num_labels = len(labels_dense)\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    index=0\n",
    "    for item in labels_dense:\n",
    "        labels_one_hot[index,item] = 1\n",
    "        #print(item)\n",
    "        #print(index,item)\n",
    "        index = index + 1\n",
    "    #print(len(labels_one_hot))\n",
    "    #print(labels_one_hot.shape)\n",
    "    #print(len(labels_one_hot[0]))\n",
    "    return labels_one_hot \n",
    "\n",
    "#Pre processing and loading the iamge dataset\n",
    "def loaddata(path, train):\n",
    "    extension = \"Bmp\"\n",
    "    image_size = 28  # Pixel width and height.\n",
    "    pixel_depth = 255.0  # Number of levels per pixel.\n",
    "    image_files = os.listdir(os.getcwd()+'/'+path+'/')\n",
    "    image_dataset = np.zeros((len(image_files), image_width, image_height))\n",
    "    \n",
    "    if train:\n",
    "        train_data = pd.read_csv(os.getcwd()+'/data/trainLabels.csv')\n",
    "        labels_flat = train_data[[1]].values.ravel()\n",
    "        image_ids =  train_data[[0]].values.ravel()\n",
    "        item = 0\n",
    "        for image_id in image_ids:\n",
    "            image_file = '' +str(image_id)+'.'+extension\n",
    "            #print (os.getcwd()+'/'+path+'/'+image_file)\n",
    "            image_data = (ndimage.imread(os.getcwd()+'/'+path+'/'+image_file).astype(float)- \n",
    "                          pixel_depth / 2) / pixel_depth\n",
    "            #resizing\n",
    "            image_data = imresize(image_data,(image_width,image_height))\n",
    "            #converting to grayscale\n",
    "            image_data = rgb2gray(image_data)\n",
    "            #displaying the last training dataset image \n",
    "            plt.imshow(image_data)\n",
    "            \n",
    "            image_dataset[item]=image_data\n",
    "            item = item + 1\n",
    "     \n",
    "        image_dataset = image_dataset.reshape([-1, image_width, image_height, 1])\n",
    "\n",
    "        image_dataset = np.multiply(image_dataset, 1.0 / pixel_depth)\n",
    "        #print(len(image_dataset))\n",
    "        #print(image_dataset.shape)\n",
    "        #print(len(image_dataset[0]))\n",
    "        #print(os.getcwd()+'/'+path+'/', image_dataset[0].shape, len(image_dataset))\n",
    "        \n",
    "        \n",
    "        # preprocessing the training image label and preforming one hot code encoding \n",
    "        labels_flat_list = []\n",
    "        label_flat_conv = []\n",
    "        for item in labels_flat :\n",
    "            labels_flat_list.append(item) \n",
    "            label_flat_conv.append(item) \n",
    "\n",
    "        labels_count = np.unique(labels_flat).shape[0]\n",
    "        \n",
    "        maplabel = []\n",
    "        for item in np.unique(labels_flat) :\n",
    "            maplabel.append(item)  \n",
    "            \n",
    "        for item in label_flat_conv :\n",
    "            pos=label_flat_conv.index(item)\n",
    "            label_flat_conv[pos]= maplabel.index(item)  \n",
    "            \n",
    "        labels = dense_to_one_hot(np.array(label_flat_conv), labels_count)\n",
    "        labels = labels.astype(np.uint8)\n",
    "\n",
    "        \n",
    "        return image_dataset, labels\n",
    "    #LOding the test dataset \n",
    "    item = 0\n",
    "    for image_file in image_files:\n",
    "\n",
    "        image_data = (ndimage.imread(os.getcwd()+'/'+path+'/'+image_file).astype(float)- \n",
    "                          pixel_depth / 2) / pixel_depth\n",
    "            \n",
    "        image_data = imresize(image_data,(image_width,image_height))\n",
    "        \n",
    "        image_data = rgb2gray(image_data)  \n",
    "        image_dataset[item] = image_data\n",
    "        item = item + 1\n",
    "     \n",
    "    image_dataset = image_dataset.reshape([-1, image_width, image_height, 1])\n",
    "\n",
    "    image_dataset = np.multiply(image_dataset, 1.0 / pixel_depth)\n",
    "    #print(len(image_dataset))\n",
    "    #print(image_dataset.shape)\n",
    "    #print(len(image_dataset[0]))\n",
    "    #print(os.getcwd()+'/'+path+'/', image_dataset[0].shape, len(image_dataset))\n",
    "        \n",
    "    return image_dataset    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Model CNN for First step with Julia\n",
    "class TFConvNet(object):\n",
    "    def __init__(self, feature_num, class_num, is_training, epochs=500000, batch_size=100, learning_rate=5e-4):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.weight_decay = 1e-3\n",
    "\n",
    "        self.bn_params = {\n",
    "            # Decay for the moving averages.\n",
    "            'decay': 0.999,\n",
    "            'center': True,\n",
    "            'scale': True,\n",
    "            # epsilon to prevent 0s in variance.\n",
    "            'epsilon': 0.001,\n",
    "            # None to force the updates during train_op\n",
    "            'updates_collections': None,\n",
    "            'is_training': is_training\n",
    "        }\n",
    "\n",
    "        self.feature_num = feature_num\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.X = tf.placeholder(tf.float32, [None, feature_num])\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, class_num])\n",
    "\n",
    "        with tf.contrib.framework.arg_scope(\n",
    "                [layers.convolution2d],\n",
    "                kernel_size=3, stride=1, padding='SAME', activation_fn=tf.nn.elu,\n",
    "                normalizer_fn=layers.batch_norm, normalizer_params=self.bn_params,\n",
    "                weights_initializer=layers.variance_scaling_initializer(),\n",
    "                weights_regularizer=layers.l2_regularizer(self.weight_decay)\n",
    "        ):\n",
    "            self.X = tf.reshape(self.X, [-1, image_width, image_height, 1])\n",
    "\n",
    "            net = layers.convolution2d(self.X, num_outputs=128)\n",
    "            net = layers.convolution2d(net, num_outputs=128)\n",
    "            net = layers.max_pool2d(net, kernel_size=2)\n",
    "            net = layers.relu(net, num_outputs=128)\n",
    "\n",
    "            net = layers.convolution2d(net, num_outputs=256)\n",
    "            net = layers.convolution2d(net, num_outputs=256)\n",
    "            net = layers.max_pool2d(net, kernel_size=2)\n",
    "            net = layers.relu(net, num_outputs=256)\n",
    "            \n",
    "            net = layers.convolution2d(net, num_outputs=512)\n",
    "            net = layers.convolution2d(net, num_outputs=512)\n",
    "            net = layers.max_pool2d(net, kernel_size=2)\n",
    "            net = layers.relu(net, num_outputs=512)\n",
    "            \n",
    "            net = layers.dropout(net, keep_prob=self.keep_prob)\n",
    "            net = layers.relu(net, num_outputs=32)\n",
    "\n",
    "            net = layers.flatten(net, [-1, 7 * 7 * 32])\n",
    "            net = layers.fully_connected(net, num_outputs=4096, activation_fn=tf.nn.relu)\n",
    "            net = layers.dropout(net, keep_prob=self.keep_prob)\n",
    "            net = layers.fully_connected(net, num_outputs=4096, activation_fn=tf.nn.relu)\n",
    "            net = layers.dropout(net, keep_prob=self.keep_prob)\n",
    "\n",
    "            net = layers.fully_connected(net, num_outputs=self.class_num, activation_fn=tf.nn.relu)\n",
    "            self.y = layers.softmax(net)\n",
    "\n",
    "        #Adam optimizer and cross entropy loss function \n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.y, self.y_))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss) \n",
    "\n",
    "        \n",
    "        pred = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "        self.acc = tf.reduce_mean(tf.cast(pred, tf.float32))\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "    #training the model    \n",
    "    def train(self, X_train, y_train, X_val, y_val, keep_prob=0.5):\n",
    "        print(\"\\nStarting to train\\n\")\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        batch_start = 0\n",
    "        batch_end = batch_start + self.batch_size\n",
    "        for iteration in range(self.epochs):\n",
    "            _, loss, probs = self.sess.run(\n",
    "                [self.optimizer, self.loss, self.y],\n",
    "                feed_dict={\n",
    "                    self.X: X_train[batch_start:batch_end],\n",
    "                    self.y_: y_train[batch_start:batch_end],\n",
    "                    self.keep_prob: keep_prob\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if iteration % 50 == 0:\n",
    "                train_acc = self.sess.run(self.acc, feed_dict={\n",
    "                    self.X: X_train[batch_start:batch_end],\n",
    "                    self.y_: y_train[batch_start:batch_end],\n",
    "                    self.keep_prob: 1.0\n",
    "                })\n",
    "\n",
    "                val_acc = self.sess.run(\n",
    "                    self.acc,\n",
    "                    feed_dict={self.X: X_val, self.y_: y_val, self.keep_prob: 1.0}\n",
    "                )\n",
    "\n",
    "                print('Iteration: {}, loss: {:2.4}, train acc: {:.3%}, validation acc: {:.3%}'.format(\n",
    "                    iteration, loss, train_acc, val_acc))\n",
    "\n",
    "                if val_acc >= 0.991:\n",
    "                    print('Validation acc good!!! Breaking the run!!!')\n",
    "                    break\n",
    "\n",
    "            batch_start = batch_end\n",
    "            batch_end += self.batch_size\n",
    "\n",
    "            if batch_end > len(X_train):\n",
    "                batch_start = 0\n",
    "                batch_end = batch_start + self.batch_size\n",
    "\n",
    "        print(\"\\nTraining ended\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y = loaddata('data/train',True)\n",
    "test_x = loaddata('data/test',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Ensuring the loaded images and one hot encoding are properly assigned.\n",
    "print('labels({0[0]},{0[1]})'.format(train_y.shape))\n",
    "print ('labels[{0}] => {1}'.format(1,train_y[6282]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image = train_x[2]\n",
    "print('image shape:', train_x.shape)\n",
    "train_x.shape\n",
    "plt.imshow(image.reshape(image_width,image_height))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2)\n",
    "print('Train size:', len(train_x))\n",
    "print('Validation size:', len(val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Let's start ...go...go..\n",
    "conv_net = TFConvNet(feature_num=(image_width*image_height), class_num=62, is_training=False)\n",
    "conv_net.train(train_x, train_y, val_x, val_y, keep_prob=0.5)\n",
    "# Note : Submission file creation code has been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
